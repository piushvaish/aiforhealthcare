{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will practice applying 2D, 2.5D and 3D convolutions to a medical volume using PyTorch. Conveniently, PyTorch offers the functionality of computing convolutions with arbitrary kernel sizes, and handles all the mechanics of striding and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy.ma as ma\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying convolutional filters\n",
    "\n",
    "In this section we will give you some starter code on how to apply 2D convolution using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 4x4 edge filter kernel\n",
    "\n",
    "conv_kernel = np.ones((4,4))\n",
    "conv_kernel[2:,:] = -1\n",
    "print(conv_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Torch's convolutional layer for our convolutional filter operation\n",
    "\n",
    "conv2d = nn.Conv2d(\n",
    "    1, # Input size == 1 (we are dealing with 1 input channel)\n",
    "    1, # Output size - we want to get 1 channel as an output\n",
    "    kernel_size = (4, 4), # size of our filter kernel\n",
    "    bias = False) # We do not need a bias for this operation\n",
    "conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's turn our convolutional kernel into a tensor which we can use to initialize our convolutional layer\n",
    "\n",
    "params = torch.from_numpy(conv_kernel).type(torch.FloatTensor).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Note the unsqeeze operation - this is effectively adding empty dimensions to the vector bringing it to 4 dimensions\n",
    "# Torch expects parameter vector of size (output_channels, input_channels, kernel x dimension, kernel y dimension)\n",
    "\n",
    "conv2d.weight = torch.nn.Parameter(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load an image of the adorable walrus:\n",
    "\n",
    "walrus = Image.open('data/walrus.jpg')\n",
    "plt.imshow(walrus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's convert it to grayscale and normalize (because this is what our convolution operation is expecting)\n",
    "\n",
    "walrus = walrus.convert(\"L\")\n",
    "walrus = np.array(walrus)\n",
    "walrus = walrus.astype(np.single)/0xff\n",
    "\n",
    "plt.imshow(walrus, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2D layer is expecting a 4D tensor (batch_size, channels, width, height)\n",
    "# Let's bring the walrus into proper shape. We have batch of size one and only one channel\n",
    "# so we will use the unsqueeze operation for this\n",
    "\n",
    "walrus_tensor = torch.from_numpy(walrus).unsqueeze(0).unsqueeze(1)\n",
    "walrus_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's compute the convolution map and ReLU activations. Note that we also used the time \"magic function\"\n",
    "# to see how long it takes. Later on, you will compare other convolution methods that you will try.\n",
    "\n",
    "convolved = conv2d(walrus_tensor)\n",
    "relu = F.relu(convolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And let's visualize them!\n",
    "\n",
    "plt.imshow(np.squeeze(convolved.detach().numpy()), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(relu.detach().numpy()), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray, we've run a 2D convolutional layer with custom kernel, using PyTorch. Onwards to medical applications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a NIFTI volume\n",
    "\n",
    "Remember how to use NiBabel to load those NIFTI volumes? Here's a refresher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load our image\n",
    "\n",
    "nii_img = nib.load(\"data/spleen.nii.gz\")\n",
    "img = nii_img.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize a slice:\n",
    "\n",
    "plt.imshow(img[:,:,0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you should've seen enough of these to recognize that you are looking at an abdominal cross-section. Now you are ready to apply convolutions!\n",
    "\n",
    "But before we go there - a couple of notes about pixel sizes, on this particular image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize sagittal cross-section at cut 250 (we will use rot90 since we want to orient the image it properly)\n",
    "\n",
    "plt.imshow(np.rot90(img[250,:,:]), cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that looks squished!\n",
    "\n",
    "Remember our conversations about anisotropic voxels ? As you might have guessed, our pixels are much shorter in z dimension than they are in x and y. Let's see what we can do about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nii_img.header[\"pixdim\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how we scaled DICOM image in previous lesson, to account for difference between in-plane resolution and resolution across z-axis? In NIFTI one extracts proper voxel aspect ratio from NIFTI image using the pixdim field.\n",
    "\n",
    "Unlike DICOM, NIFTI files store all their pixel dimensions in a single place - pixdim field. Also unlike DICOM this field also stores a bunch of other stuff related to uses of NIFTI format beyond static 3D images. If you're curious what those are - check out the [NIFTI documentation](https://nifti.nimh.nih.gov/nifti-1/documentation/nifti1fields/nifti1fields_pages/pixdim.html/document_view). \n",
    "\n",
    "For the purpose of this exercise we are interested in values at locations 1, 2 and 3 - these are our x, y and z dimensions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img2d = np.rot90(img[250,:,:])\n",
    "plt.imshow(img2d, cmap = \"gray\", aspect=nii_img.header[\"pixdim\"][3]/nii_img.header[\"pixdim\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also give you a convenience function to visualize all slices in a grid as thumbnails:\n",
    "\n",
    "def display_volume_slices(img, w, h):\n",
    "    plot_w = w\n",
    "    plot_h = h\n",
    "\n",
    "    # You can play with figsize parameter to adjust how large the images are\n",
    "    fig, ax = plt.subplots(plot_h, plot_w, figsize=[35,35])\n",
    "\n",
    "    for i in range(plot_w*plot_h):\n",
    "        plt_x = i % plot_w\n",
    "        plt_y = i // plot_w\n",
    "        if (i < len(img)):\n",
    "            ax[plt_y, plt_x].set_title(f\"slice {i}\")\n",
    "            ax[plt_y, plt_x].imshow(img[i], cmap='gray')\n",
    "        ax[plt_y, plt_x].axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# And let's visualize ALL slices\n",
    "#\n",
    "# Note that we are calling np.transpose here because our display_volume_slices iterates over 0th dimension\n",
    "# of the input volume. Our Nibabel volumes have z dimension stored in the 2nd position, so we \n",
    "# move the z-dimension in front of the other two here\n",
    "\n",
    "display_volume_slices(np.transpose(img, (2, 0, 1)), 7, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2D Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Using the kernel we have defined above (or any other kernel you like), use the \"2D Convolution\" method to create a volume of convolution maps for all of the slices in our volume. Visualize them.\n",
    "\n",
    "Use %%time (as we've done above) to see how much time it takes.  \n",
    "How many parameters does define the convolution operation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 3D Convolutions\n",
    "\n",
    "**TASK**: Using a 3D version of the same kernel, compute and time full 3D convolutions by using PyTorch's Conv3D layer. Note that our kernel represents a 2D edge filter. What would be a 3D edge filter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 2.5D Convolutions\n",
    "\n",
    "> **Extra credit exercise**: _Depending on how familiar you are with Numpy and PyTorch, this exercise may take up to a few hours to complete. This exercise may help you understand the architecture of convolutions better, but is not essential to understanding the course. If you feel like this would be too time consuming you are welcome to move on or take a peek at the solution._\n",
    "\n",
    "Now Let's try 2.5D convolutions. That's a bit more difficult since we want to be specific about how exactly we select data for our extra planes, and also for it to be meaningful we would like to combine data in small areas around area of interest which means we would need to process image in patches. This is not something PyTorch offers right out of the box, and you would engage in something like this if you are conscious of performance, so you would want to have control over this anyway. A bit more coding will be required than previous exercises. \n",
    "\n",
    "We will try to recreate the approach to building convolutions which has been presented in [this paper from the NIH](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4295635/). Specifically, we will build a layer that runs a convolutional filter over patches that look like this:\n",
    "\n",
    "<img src=\"convolutions.img/25d.jpg\" width=\"200\">\n",
    "\n",
    "**TASK**: Using same kernel, compute and time creation of a volume of convolutional feature maps using the 2.5D convolutions approach by looking at 16x16x16 patches and extracting three square segments from the center of the patch, along the cardinal planes, similar to how it was described in the lesson video. Treat the three 16x16 segments as input channels for your convolutional layer still using one output channel). Visualize all axial slices of your convolutional feature map.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
